{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SC2PATH\"] = \"/home/nsml/StarCraftII\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    \n",
    "    flags.DEFINE_bool(\"render\", False, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", None, sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", False,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", False, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent\", \"pysc2.agents.random_agent.RandomAgent\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"random\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"random\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "    \n",
    "    flags.DEFINE_string(\"map\", \"CollectMineralShards\", \"Name of a map to use.\")\n",
    "    \n",
    "    \n",
    "flags_defined = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    def __init__(self, M1, M2, f=tf.nn.tanh, use_bias=True):\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(M1, M2)))\n",
    "        self.params = [self.W]\n",
    "        self.use_bias = use_bias\n",
    "        if use_bias:\n",
    "            self.b = tf.Variable(np.zeros(M2).astype(np.float32))\n",
    "            self.params.append(self.b)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.use_bias:\n",
    "            a = tf.matmul(X, self.W) + self.b\n",
    "        else:\n",
    "            a = tf.matmul(X, self.W)\n",
    "        return self.f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, D, K, hidden_layer_sizes, gamma, max_experiences=10000, min_experiences=100, batch_sz=32):\n",
    "        self.K = K\n",
    "\n",
    "        # create the graph\n",
    "        self.layers = []\n",
    "        M1 = D\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.layers.append(layer)\n",
    "            M1 = M2\n",
    "\n",
    "        # final layer\n",
    "        layer = HiddenLayer(M1, K, lambda x: x)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        # collect params for copy\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "        # inputs and targets\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "        self.G = tf.placeholder(tf.float32, shape=(None,), name='G')\n",
    "        self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n",
    "\n",
    "        # calculate output and cost\n",
    "        Z = self.X\n",
    "    for layer in self.layers:\n",
    "        Z = layer.forward(Z)\n",
    "    Y_hat = Z\n",
    "    self.predict_op = Y_hat\n",
    "\n",
    "    selected_action_values = tf.reduce_sum(\n",
    "        Y_hat * tf.one_hot(self.actions, K),\n",
    "        reduction_indices=[1]\n",
    "        )\n",
    "\n",
    "    cost = tf.reduce_sum(tf.square(self.G - selected_action_values))\n",
    "    self.train_op = tf.train.AdamOptimizer(1e-2).minimize(cost)\n",
    "    # self.train_op = tf.train.AdagradOptimizer(1e-2).minimize(cost)\n",
    "    # self.train_op = tf.train.MomentumOptimizer(1e-3, momentum=0.9).minimize(cost)\n",
    "    # self.train_op = tf.train.GradientDescentOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "    # create replay memory\n",
    "    self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n",
    "    self.max_experiences = max_experiences\n",
    "    self.min_experiences = min_experiences\n",
    "    self.batch_sz = batch_sz\n",
    "    self.gamma = gamma\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def copy_from(self, other):\n",
    "        # collect all the ops\n",
    "        ops = []\n",
    "        my_params = self.params\n",
    "        other_params = other.params\n",
    "        for p, q in zip(my_params, other_params):\n",
    "            actual = self.session.run(q)\n",
    "            op = p.assign(actual)\n",
    "            ops.append(op)\n",
    "    # now run them all\n",
    "    self.session.run(ops)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: X})\n",
    "\n",
    "    def train(self, target_network):\n",
    "    # sample a random batch from buffer, do an iteration of GD\n",
    "        if len(self.experience['s']) < self.min_experiences:\n",
    "        # don't do anything if we don't have enough experience\n",
    "        return\n",
    "\n",
    "        # randomly select a batch\n",
    "        idx = np.random.choice(len(self.experience['s']), size=self.batch_sz, replace=False)\n",
    "        # print(\"idx:\", idx)\n",
    "        states = [self.experience['s'][i] for i in idx]\n",
    "        actions = [self.experience['a'][i] for i in idx]\n",
    "        rewards = [self.experience['r'][i] for i in idx]\n",
    "        next_states = [self.experience['s2'][i] for i in idx]\n",
    "        dones = [self.experience['done'][i] for i in idx]\n",
    "        next_Q = np.max(target_network.predict(next_states), axis=1)\n",
    "        targets = [r + self.gamma*next_q if not done else r for r, next_q, done in zip(rewards, next_Q, dones)]\n",
    "\n",
    "        # call optimizer\n",
    "        self.session.run(\n",
    "            self.train_op,\n",
    "            feed_dict={\n",
    "                self.X: states,\n",
    "                self.G: targets,\n",
    "                self.actions: actions\n",
    "                }\n",
    "        )\n",
    "\n",
    "    def add_experience(self, s, a, r, s2, done):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            self.experience['s'].pop(0)\n",
    "            self.experience['a'].pop(0)\n",
    "            self.experience['r'].pop(0)\n",
    "            self.experience['s2'].pop(0)\n",
    "            self.experience['done'].pop(0)\n",
    "            self.experience['s'].append(s)\n",
    "            self.experience['a'].append(a)\n",
    "            self.experience['r'].append(r)\n",
    "            self.experience['s2'].append(s2)\n",
    "            self.experience['done'].append(done)\n",
    "\n",
    "    def sample_action(self, x, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            X = np.atleast_2d(x)\n",
    "            return np.argmax(self.predict(X)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(base_agent.BaseAgent):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(DQNAgent, self).step(obs)\n",
    "        \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 10:53:00.262745 140179313997632 sc_process.py:135] Launching SC2: /home/nsml/StarCraftII/Versions/Base59877/SC2_x64 -listen 127.0.0.1 -port 18673 -dataDir /home/nsml/StarCraftII/ -tempDir /tmp/sc-bh3xpzok/\n",
      "I1212 10:53:00.273363 140179313997632 remote_controller.py:167] Connecting to: ws://127.0.0.1:18673/sc2api, attempt: 0, running: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CollectMineralShards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 10:53:01.288214 140179313997632 remote_controller.py:167] Connecting to: ws://127.0.0.1:18673/sc2api, attempt: 1, running: True\n",
      "I1212 10:53:02.292108 140179313997632 remote_controller.py:167] Connecting to: ws://127.0.0.1:18673/sc2api, attempt: 2, running: True\n",
      "I1212 10:53:04.193245 140179313997632 sc2_env.py:752] Environment Close\n",
      "I1212 10:53:04.311958 140179313997632 sc_process.py:232] Shutdown gracefully.\n",
      "I1212 10:53:04.312608 140179313997632 sc_process.py:210] Shutdown with return code: -2\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "    \n",
    "    agent = DQNAgent('a')\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            FLAGS.map = \"CollectMineralShards\"\n",
    "            print(FLAGS.map)\n",
    "            \n",
    "            with sc2_env.SC2Env(\n",
    "                map_name=FLAGS.map,\n",
    "                players=[sc2_env.Agent(sc2_env.Race.terran)],\n",
    "                agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "                  feature_screen=FLAGS.feature_screen_size,\n",
    "                  feature_minimap=FLAGS.feature_minimap_size,\n",
    "                  rgb_screen=FLAGS.rgb_screen_size,\n",
    "                  rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "                  action_space=FLAGS.action_space,\n",
    "                  use_feature_units=FLAGS.use_feature_units),\n",
    "                step_mul=FLAGS.step_mul,\n",
    "                game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "                disable_fog=FLAGS.disable_fog,\n",
    "                visualize=FLAGS.render) as env:\n",
    "\n",
    "                    agent.setup(env.observation_spec(), env.action_spec())\n",
    "\n",
    "                    timesteps = env.reset()\n",
    "                    agent.reset()\n",
    "                    \n",
    "                    print(FLAGS.max_episodes)\n",
    "\n",
    "                    run_loop.run_loop([agent], env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
